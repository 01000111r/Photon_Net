{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5121f66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "from numpy.random import default_rng\n",
    "from functools import partial\n",
    "import time\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "# Fix a Random Number Generator for reproducibility\n",
    "rng = default_rng(1337)\n",
    "from thewalrus import perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aa75d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code implements a photonic circuit with trainable phases,\n",
    "\n",
    "def rescale_data(data_set, min_val = -(np.pi)/2, max_val = (np.pi)/2):\n",
    "    \"\"\"\n",
    "    Rescales the data set to the range [min_val, max_val].\n",
    "    \"\"\"\n",
    "    min_data = np.copysign(np.ceil(np.abs(jnp.min(data_set))), jnp.min(data_set))\n",
    "    max_data = np.copysign(np.ceil(np.abs(jnp.max(data_set))), jnp.max(data_set))\n",
    "    \n",
    "    # Rescale the data to the range [-pi/2, pi/2]\n",
    "    rescaled_data = (data_set - min_data) / (max_data - min_data)\n",
    "    # Now scale it to the desired range\n",
    "    rescaled_data = rescaled_data * (max_val - min_val) + min_val\n",
    "    return rescaled_data\n",
    "\n",
    "# Initializing phases over the full interval [0,2pi] invites barren plateaus. It is much  \n",
    "# better to initialize close to 0.\n",
    "\n",
    "# This is the function used for loss calculation /predictions etc\n",
    "# we need to inlcude only trainable phases here\n",
    "# Considering alterante layers of data reuploading \n",
    "\n",
    "def initialize_phases(depth, width = None, mask = None):\n",
    "    # Default case is Clements et al. layout, with all beam splitters tunable.\n",
    "    if width == None:\n",
    "        width = depth \n",
    "    if mask == None:\n",
    "        #mask = np.ones(shape = [depth//2, width//2 -1, 2])\n",
    "        mask = np.zeros((depth, width // 2, 2))\n",
    "        for i in range(1, depth, 2):  # odd layers only\n",
    "            mask[i] = 1.0\n",
    "\n",
    "    # // 2 is integer division by 2, including rounding down.\n",
    "    # The last two says that these two phases belong  to the same beamsplitter.\n",
    "    # That is also why we divide the width by 2.\n",
    "    phases = rng.uniform( low = -0.1, high = 0.1 , size =  [depth, width//2, 2])\n",
    "    # The mask allows to set some phases to zero. This can be used if one wants to \n",
    "    # fix some beam splitters to the identity, for example for modularity.\n",
    "    phases = mask*phases   \n",
    "    phases = jnp.array(phases)\n",
    "    return phases\n",
    "\n",
    "\n",
    "# JAX likes to complain A LOT. Therefore, we need to explicitly declare variables as static.\n",
    "# E.g. JAX doesn't like to use shapes of trainable arrays to define new arrays, or anything really.     \n",
    "#@jax.jit\n",
    "@partial(jax.jit, static_argnames=['layer'])\n",
    "def layer_unitary(all_phases, layer, mask = None):\n",
    "    \"\"\" \n",
    "    Returns a layer unitary implementing the optical layer of the trainable part with index 'layer'.\n",
    "    \"\"\"\n",
    "    #layer = jax.lax.stop_gradient(layer) # doesn't work, don't ask me why.\n",
    "    \n",
    "    width = 2*jax.lax.stop_gradient(all_phases).shape[1] \n",
    "    # Stopping the gradient here allows to use the size of an input tensor to define other tensors.\n",
    "    # Depth of the trainable part.\n",
    "    depth = jax.lax.stop_gradient(all_phases).shape[0]\n",
    "    if mask == None:\n",
    "        # The default mask allows all phases to be trained\n",
    "        mask = jnp.ones(shape = [depth, width//2])\n",
    "    trainable_layer_phases = jnp.zeros(shape = [width//2 , 2])\n",
    "    # Notice that the scalar operation '*' is applied for each beamsplitter of the layer individually, \n",
    "    # hidden in the fact that mask[layer] still has a dimension for the width, and the ':'.\n",
    "    trainable_layer_phases = trainable_layer_phases.at[:, 0].set(mask[layer]*all_phases[layer,:,0])\n",
    "    trainable_layer_phases = trainable_layer_phases.at[:, 1].set(mask[layer]*all_phases[layer,:,1])\n",
    "    \n",
    "    unitary = jnp.zeros(shape = [width, width], dtype = jnp.complex64)\n",
    "    # Odd layers get an offset of one for placing beamsplitters.\n",
    "    \n",
    "    offset = (layer) % 2\n",
    "    # Take care of wires that do not see a beamsplitter in this layer.\n",
    "    if offset == 1:\n",
    "        unitary = unitary.at[0,0].set(1.0)\n",
    "        # If the width is even and the offset is one, also the last wire does not get a beamsplitter.\n",
    "        if width % 2 == 0:\n",
    "            # -1 gives the last entry\n",
    "            unitary = unitary.at[-1, -1].set(1.0)\n",
    "    else:  # Offset is 0, so for odd number of wires the last one cannot get a beamsplitter.  \n",
    "        if width % 2 == 1:\n",
    "            unitary = unitary.at[-1,-1].set(1.0)  \n",
    "            \n",
    "    # Now, write the actual layers\n",
    "    # Since the entries look so different, I have no clever idea for how to vectorize/broadcast this...\n",
    "    for index in range( (width-offset)//2):\n",
    "        p = trainable_layer_phases[index,0]\n",
    "        q = trainable_layer_phases[index,1]\n",
    "        # Taken from old code. However, for p=q=0, it does not give the identity. Therefore the mask doesn't work.\n",
    "        #unitary = unitary.at[offset+2*index, offset+2*index].set(jnp.exp(p*1j)*jnp.sin(q/2))\n",
    "        #unitary = unitary.at[offset+2*index , offset+2*index+1].set(jnp.cos(q/2))\n",
    "        #unitary =  unitary.at[offset+2*index+1, offset+2*index].set(jnp.exp(p*1j)*jnp.cos(q/2))\n",
    "        #unitary = unitary.at[offset+2*index+1, offset+2*index+1].set(-jnp.sin(q/2))\n",
    "        \n",
    "        # To get the mask to work, I am using a different parameterization.\n",
    "        unitary = unitary.at[offset+2*index, offset+2*index].set(0.5*(1+jnp.exp(1j*p)))\n",
    "        unitary = unitary.at[offset+2*index , offset+2*index+1].set(0.5*(jnp.exp(1j*q)-jnp.exp(1j*(q+p))))\n",
    "        unitary = unitary.at[offset+2*index+1, offset+2*index].set(0.5*(1-jnp.exp(1j*p)))\n",
    "        unitary = unitary.at[offset+2*index+1, offset+2*index+1].set(0.5*(jnp.exp(1j*q)+jnp.exp(1j*(q+p))))\n",
    "        \n",
    "\n",
    "    # Taken from old code to remind myself.\n",
    "    #splitters = jnp.array([[jnp.exp(p*1j)*np.sin(q/2), np.cos(q/2)], [np.exp(p*1j)*np.cos(q/2), -np.sin(q/2)]] for q,p in [q_all, p_all])\n",
    "\n",
    "    return unitary\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def data_upload(data_set):\n",
    "    \"\"\" Uploads the images into the photonics circuit \"\"\"\n",
    "    num_samples = jax.lax.stop_gradient(data_set).shape[0]\n",
    "    # Each pixel gets its BS, therefore factor 2\n",
    "    width = 2*jax.lax.stop_gradient(data_set).shape[1]\n",
    "    # Again, the 3rd dimension with 2 represents the two phases for each beamsplitter. \n",
    "    phases = (jnp.pi/2)*jnp.ones(shape = [num_samples, width//2, 2]) \n",
    "    # The first of the phases of the beam splitters are set to be the feature values, the second phases are set \n",
    "    # to a constant pi/2 . 0 doesn't work because minimal and maximal pixel brightness act on the uniform superposition\n",
    "    # state identically, with the parameterization below. For q = pi/2, minimal and maximal pixel brightness move the\n",
    "    # uniform superposition into orthogonal states.  \n",
    "    phases = phases.at[:,:,0].set(data_set)\n",
    "\n",
    "    # Note that our \"unitary\" has 3 dimensions. The 1st dimension is a batching dimension, \n",
    "    # representing the index of the image. This allows to parallelize the calculation of hthe loss \n",
    "    # over the full training set later.\n",
    "    unitary = jnp.zeros(shape = [num_samples, width, width], dtype = jnp.complex64)    \n",
    "    for index in range( width//2 ):    \n",
    "        #print('yes')\n",
    "        p = phases[:, index, 0]\n",
    "        q = phases[:, index, 1]\n",
    "        # Note that p and q are 1-dimensional tensors here. We use that all operations here like jnp.exp are \n",
    "        # applied entry-by-entry to calculate the uploading unitary for all images in parallel.\n",
    "        # That means each entry of p and q corresponds to one image, which corresponds to \n",
    "        # one entry in the : in dimension 0. \n",
    "        unitary = unitary.at[:,2*index, 2*index].set(0.5*(1+jnp.exp(1j*p)))\n",
    "        unitary = unitary.at[:,2*index , 2*index+1].set(0.5*(jnp.exp(1j*q)-jnp.exp(1j*(q+p))))\n",
    "        unitary = unitary.at[:,2*index+1, 2*index].set(0.5*(1-jnp.exp(1j*p)))\n",
    "        unitary = unitary.at[:,2*index+1, 2*index+1].set(0.5*(jnp.exp(1j*q)+jnp.exp(1j*(q+p))))\n",
    "\n",
    "    return unitary\n",
    "\n",
    "def perm_3x3_jax(mat):\n",
    "    # Only works for 3x3 matrices\n",
    "    perms = jnp.array([\n",
    "        [0, 1, 2],\n",
    "        [0, 2, 1],\n",
    "        [1, 0, 2],\n",
    "        [1, 2, 0],\n",
    "        [2, 0, 1],\n",
    "        [2, 1, 0]\n",
    "    ])\n",
    "    return jnp.sum(jnp.prod(mat[jnp.arange(3), perms], axis=1))\n",
    "\n",
    "\n",
    "\n",
    "def measurement(unitaries, num_photons=3):\n",
    "    n = unitaries.shape[0]\n",
    "    num_modes = unitaries.shape[1]\n",
    "    out_state_combos = jnp.array(list(combinations(range(num_modes), num_photons)))\n",
    "    n_combos = out_state_combos.shape[0]\n",
    "\n",
    "    parity_out_state_combos = jnp.sum(out_state_combos, axis=1) % 2\n",
    "\n",
    "    # Truncate to first 3 columns\n",
    "    #print('Unitaries', unitaries)\n",
    "    input_state_modes = jnp.array([0, num_modes//2, num_modes-1])\n",
    "    unitaries_truncated = unitaries[:, :, input_state_modes]\n",
    "    #print('Unitaries truncated', unitaries_truncated)\n",
    "    # Vectorized extraction of submatrices for all samples and all combos\n",
    "    def extract_submatrices(unitary):\n",
    "        # unitary: (num_modes, 3)\n",
    "        # out_state_combos: (n_combos, 3)\n",
    "        return unitary[out_state_combos, :]  # (n_combos, 3, 3)\n",
    "\n",
    "    # Apply to all samples--\n",
    "    all_extracts = jax.vmap(extract_submatrices)(unitaries_truncated)  # (n, n_combos, 3, 3)\n",
    "\n",
    "    # Vectorized permanent calculation over all submatrices\n",
    "    perm_fn = jax.vmap(lambda mat: jnp.abs(perm_3x3_jax(mat))**2)\n",
    "    all_probs = jax.vmap(perm_fn, in_axes=0)(all_extracts)  # (n, n_combos)\n",
    "    #print(all_probs[:10])\n",
    "\n",
    "    ##\n",
    "    plus_1_probs = all_probs * parity_out_state_combos\n",
    "    plus_minus1_probs = all_probs * (1 - parity_out_state_combos)\n",
    "\n",
    "    # Sum over all output state probabilities for each sample\n",
    "    total_probs = jnp.sum(all_probs, axis=1, keepdims=True)  # shape (n, 1)\n",
    "\n",
    "    # Normalise all probabilities\n",
    "    all_probs_norm = all_probs / total_probs\n",
    "    plus_1_probs_norm = plus_1_probs / total_probs\n",
    "    plus_minus1_probs_norm = plus_minus1_probs / total_probs\n",
    "\n",
    "    # Normalised binary probabilities\n",
    "    binary_probs = jnp.sum(plus_1_probs_norm, axis=1, keepdims=True)\n",
    "    binary_probs_minus = jnp.sum(plus_minus1_probs_norm, axis=1, keepdims=True)\n",
    "\n",
    "    # Optionally print to check\n",
    "    #print('Sum of all_probs_norm (should be 1):', jnp.sum(all_probs_norm, axis=1))\n",
    "    #print('Sum of binary_probs + binary_probs_minus (should be 1):', (binary_probs + binary_probs_minus).squeeze())\n",
    "    \n",
    "    ##\n",
    "\n",
    "    \n",
    "    # 0 = even (-1), 1 = odd (+1)\n",
    "    #even_indices = jnp.where(parity_out_state_combos == 0)[0]\n",
    "    \n",
    "    #plus_1_probs = all_probs*parity_out_state_combos\n",
    "    #plus_minus1_probs = all_probs*(1-parity_out_state_combos)\n",
    "    #print(plus_1_probs[:10, :5])\n",
    "    #binary_probs = jnp.sum(plus_1_probs, axis=1, keepdims=True)\n",
    "    #binary_probs_minus = jnp.sum(plus_minus1_probs, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "    #total_probs = binary_probs + binary_probs_minus\n",
    "    #print('total_probs', total_probs[:10])\n",
    "    #print('minus', binary_probs_minus[:10])  \n",
    "    #print('plus',binary_probs[:10])\n",
    "    #print('plus total sum', jnp.sum(binary_probs))\n",
    "    #print('minus total sum', jnp.sum(binary_probs_minus))\n",
    "  \n",
    "    return all_extracts, out_state_combos, all_probs, binary_probs\n",
    "\n",
    "#key = jax.random.PRNGKey(12)\n",
    "def full_unitaries_data_reupload(phases, data_set, weights):\n",
    "    # Depth of the trainable part. \n",
    "    depth = jax.lax.stop_gradient(phases).shape[0]\n",
    "\n",
    "    # Please note that we broadcast over images in the data set. \n",
    "    # The convention is that only the last two indices are used for matrix operations, \n",
    "    # the others are broadcasting dimensions used for batches of images.\n",
    "\n",
    "    first_layers = data_upload(weights[0,:]*data_set)\n",
    "    unitaries = first_layers\n",
    "    \n",
    "    for layer in range(1,depth): \n",
    "        \n",
    "        if (layer)%4 != 0: # every 4th layer is a trainable layer \n",
    "            unitaries = layer_unitary(phases, layer) @ unitaries\n",
    "        # 'layer' is the layer index in the trainable part, starting from 0.\n",
    "        else:        \n",
    "\n",
    "            key = jax.random.PRNGKey(layer) \n",
    "            temp = jax.random.permutation(key, data_set.shape[1])\n",
    "            temp = jax.lax.stop_gradient(temp)\n",
    "            #temp = jnp.arange(data_set.shape[0])\n",
    "            data_set_reupload = data_set[:,temp]\n",
    "            \n",
    "            #temp_permutation = data_set_reupload[:10, :3]\n",
    "            #print(temp_permutation)\n",
    "\n",
    "\n",
    "            unitaries_data_reupload = data_upload(weights[layer,:]* data_set_reupload)\n",
    "            unitaries = unitaries_data_reupload @ unitaries\n",
    "\n",
    " \n",
    "\n",
    "    # Extract the probabilities of the output states.\n",
    "    sub_unitaries, _, label_probs, binary_probs_plus = measurement(unitaries, num_photons = 3)\n",
    "    #print(label_probs[:10, :])\n",
    "    #print(binary_probs_plus[:10,:])\n",
    "   \n",
    "    return unitaries, sub_unitaries, label_probs, binary_probs_plus\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def predict_reupload(phases, data_set, weights):\n",
    "\n",
    "    _, _, probs, binary_probs_plus = full_unitaries_data_reupload(phases, data_set, weights)\n",
    "    adjusted_binary_probs = jnp.where(binary_probs_plus > 0.5, binary_probs_plus,  - binary_probs_plus)\n",
    "    \n",
    "    return probs, adjusted_binary_probs\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def loss(phases, data_set, labels, weights):\n",
    "    num_samples = jax.lax.stop_gradient(data_set).shape[0]\n",
    "    _, binary_predictions_plus = predict_reupload(phases, data_set, weights)\n",
    "\n",
    "\n",
    "    binary_predictions_plus = binary_predictions_plus.squeeze() # to match shapes\n",
    "    binary_predictions_plus = jnp.abs(binary_predictions_plus) # as mentioned previously, we dont use the negative probs\n",
    "    # Adjust predictions based on labels: if label == +1, keep it; else flip it\n",
    "    adjusted_predictions = jnp.where(labels == 1, binary_predictions_plus, (1.0 - (binary_predictions_plus)))\n",
    "\n",
    "\n",
    "    loss = ((1.0- adjusted_predictions)**2).mean()\n",
    "  \n",
    "    return loss\n",
    "\n",
    "\n",
    "# JAX itself just calculates gradients, but doesn't come with an optimizer.\n",
    "# So I took the Adam optimizer from the old code.\n",
    "# Also, to prevent the kernel from crashing, I had to use jax.lax.scan here instead of for-loops.\n",
    "# And that requires writing a function for the loop iteration, and put most of the variables into a \n",
    "# list called 'carry'... Thanks, JAX.\n",
    "@jax.jit\n",
    "def adam_step(carry, step):\n",
    "    params_phases, data_set, labels, params_weights, m_phases, v_phases, m_weights, v_weights = carry\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    eps = 1e-8\n",
    "    eta = step_size\n",
    "\n",
    "    loss_val = jax.lax.stop_gradient(loss(params_phases, data_set, labels, params_weights))\n",
    "    grad_params, grad_weights = jax.grad(loss, argnums=(0, 3))(params_phases, data_set, labels, params_weights)\n",
    "\n",
    "    # Update params\n",
    "    m_phases = beta1 * m_phases + (1 - beta1) * jax.lax.stop_gradient(grad_params)\n",
    "    v_phases = beta2 * v_phases + (1 - beta2) * jax.lax.stop_gradient(grad_params)**2\n",
    "    m_hat_params = m_phases / (1 - beta1**step)\n",
    "    v_hat_params = v_phases / (1 - beta2**step)\n",
    "    params_phases = params_phases - eta * m_hat_params / (jnp.sqrt(v_hat_params) + eps)\n",
    "\n",
    "    # Update weights\n",
    "    m_weights = beta1 * m_weights + (1 - beta1) * jax.lax.stop_gradient(grad_weights)\n",
    "    v_weights = beta2 * v_weights + (1 - beta2) * jax.lax.stop_gradient(grad_weights)**2\n",
    "    m_hat_weights = m_weights / (1 - beta1**step)\n",
    "    v_hat_weights = v_weights / (1 - beta2**step)\n",
    "    params_weights = params_weights - eta * m_hat_weights / (jnp.sqrt(v_hat_weights) + eps)\n",
    "\n",
    "    step = step + 1\n",
    "    carry = [params_phases, data_set, labels, params_weights, m_phases, v_phases, m_weights, v_weights]\n",
    "    return carry, jnp.array([step, loss_val])\n",
    "\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train(init):\n",
    "    # Using for-loops ate all my RAM and then the kernel crashed.\n",
    "    # But this awkward scan-loop seems to work very well!\n",
    "    steps = jnp.arange(num_steps)+1\n",
    "    carry, loss_mem = jax.lax.scan(adam_step, init, steps)\n",
    "    # Note that jax.lax.scan automatically stacks the [step, loss_val] tensors we output\n",
    "    # at the end of every Adam step. Therefore, the second output of scan is already\n",
    "    # the full memory of all losses.\n",
    "    return carry, loss_mem\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16072754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data for handwritten digits '3' and '5'\n",
    "# The dataset has 'x' rows, each representing an image\n",
    "# Each row has 'y' columns: The first y-1 columns each correspond to a pixel feature, and the last column is the label\n",
    "# A label of +1 represents the digit '3', and -1 represents the digit '5'\n",
    "\n",
    "file_path = r\"/Users/giancarloramirez/documents/qml/mnist_pca/mnist_3-5_8d_test.csv\"\n",
    "data_train = pd.read_csv(file_path)\n",
    "data_train = jnp.array(data_train)\n",
    "\n",
    "\n",
    "#final column in the data is for labels \n",
    "num_features = data_train.shape[1] -1 \n",
    "\n",
    "file_path_1 = r\"/Users/giancarloramirez/documents/qml/mnist_pca/mnist_3-5_8d_test.csv\"\n",
    "data_test = pd.read_csv(file_path_1)\n",
    "data_test = jnp.array(data_test)\n",
    "\n",
    "num_steps = 10\n",
    "\n",
    "\n",
    "# Spliiting the data into training and testing sets\n",
    "\n",
    "train_set = data_train[:,:num_features]\n",
    "\n",
    "# Rescale the training set to the range [-pi/2, pi/2]\n",
    "train_set = rescale_data(train_set, min_val = -(np.pi)/2, max_val = (np.pi/2))\n",
    "\n",
    "# The training labels are +1 for 3 and -1 for 5. \n",
    "train_labels = data_train[:,num_features]\n",
    "\n",
    "# Not needed for now:\n",
    "#train_labels_one_hot = jnp.zeros(shape = train_set.shape)\n",
    "#train_labels_one_hot = train_labels_one_hot.at[tuple([jnp.arange(num_train),train_labels])].set(1)\n",
    "test_set = data_test[:,:num_features]\n",
    "# Rescale the test set to the range [-pi/2, pi/2]\n",
    "test_set = rescale_data(test_set, min_val = -(np.pi)/2, max_val = (np.pi/2))\n",
    "\n",
    "test_labels = data_test[:,num_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4964d74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1901, 8) (1901,) (1901, 8) (1901,)\n",
      "(1901, 16, 16)\n",
      "(1901, 560, 3, 3)\n",
      "(1901, 560)\n",
      "7.390085935592651\n",
      "(1901, 16, 16)\n",
      "(1901, 560, 3, 3)\n",
      "(1901, 560)\n",
      "0.07079291343688965\n"
     ]
    }
   ],
   "source": [
    "# Not needed for now:\n",
    "\n",
    "print( train_set.shape, train_labels.shape, test_set.shape, test_labels.shape)\n",
    "\n",
    "# Initialize the phases\n",
    "\n",
    "# Each feature gets its own mode. (previously, each feature had its own uploading BS so we had a factor of 2)\n",
    "init_phases = initialize_phases(2*num_features, )  #quick check - multiply n * m to know total variable BS -- checks out for odd feature sizes and puts extra bs for even -- need fix \n",
    "\n",
    "weights_data = jnp.ones(shape = [init_phases.shape[0],init_phases.shape[1]]) #weights for data reuploading\n",
    "#print(init_phases)\n",
    "#print(init_phases)\n",
    "\n",
    "# If you didn't test any of the jitted functions yet, the ratio in times should be around a factor 10^3 - 10^5.\n",
    "# The first time is larger because of the compilation.\n",
    "# The second time is small because it just runs the compiled code.\n",
    "# Also, try to get any of these run times in pure Python+Numpy.\n",
    "b = time.time()\n",
    "# The block_until_ready is supposed to only let Python continue when the compiled code has finished.\n",
    "# For me, it's not reliable. Therefore, I print the results first before measuring the end time.\n",
    "result1, result2, result3, x = jax.block_until_ready(full_unitaries_data_reupload)(init_phases, train_set, weights_data)\n",
    "print(result1.shape)\n",
    "print(result2.shape)\n",
    "print(result3.shape)\n",
    "e = time.time()\n",
    "print(e-b)\n",
    "b = time.time()\n",
    "result1 , result2, result3, x  = jax.block_until_ready(full_unitaries_data_reupload)(init_phases, train_set, weights_data)\n",
    "print(result1.shape)\n",
    "print(result2.shape)\n",
    "print(result3.shape)\n",
    "e = time.time()\n",
    "print(e-b)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7085662b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 1.0954572359720867 minutes\n",
      "Loss on train set before training: 0.3429857\n",
      "Loss on train set after training: 0.26978317\n",
      "Loss on test set after training: 0.26978317\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 1.2654783487319947 minutes\n"
     ]
    }
   ],
   "source": [
    "#Training the circuit\n",
    "\n",
    "a = time.time()\n",
    "step_size = 1e-2\n",
    "# Initialize the carry for Adam.\n",
    "init = [init_phases, train_set, train_labels, weights_data, 0.0*init_phases, 0.0*init_phases, 0.0*weights_data, 0.0*weights_data] \n",
    "# [params_phases, data_set, labels, params_weights, m_phases, v_phases, m_weights, v_weights]\n",
    "\n",
    "# Run the training.\n",
    "b = time.time()\n",
    "carry, loss_mem = jax.block_until_ready(train)(init)\n",
    "trained_phases = carry[0]\n",
    "trained_weights = carry[3]\n",
    "#print(loss_mem[0])\n",
    "#print(loss_mem[10])\n",
    "#print(loss_mem[100])\n",
    "#print(loss_mem[1000])\n",
    "e = time.time()\n",
    "print('Time', (e-b)/60, 'minutes')\n",
    "\n",
    "\n",
    "temp_loss = loss(init_phases, train_set, train_labels, weights_data)\n",
    "print('Loss on train set before training:', temp_loss)\n",
    "\n",
    "temp_loss = loss(trained_phases, train_set, train_labels, trained_weights)\n",
    "print('Loss on train set after training:', temp_loss)\n",
    "\n",
    "temp_loss = loss(trained_phases, test_set, test_labels, trained_weights)\n",
    "# Let's test on our test data:\n",
    "print('Loss on test set after training:', temp_loss)\n",
    " \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.array(loss_mem[:,0]), np.array(loss_mem[:,1]), linewidth=2, label=r\"learning curve\" )\n",
    "ax.set(xlim = (0,1000), xlabel=\"step\", ylabel = \"loss\",)\n",
    "#ax.legend(fontsize=14)\n",
    "unitary_shape = trained_phases.shape  \n",
    "ax.set_title(f\"Unitary dimension: {2*unitary_shape[1]} x {2*unitary_shape[1]} \")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "d = time.time()\n",
    "print('Total time:', (d-a)/60, 'minutes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91356c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with depth = 8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     53\u001b[39m init = [\n\u001b[32m     54\u001b[39m     init_phases, train_set, train_labels, weights_data,\n\u001b[32m     55\u001b[39m     \u001b[32m0.0\u001b[39m*init_phases, \u001b[32m0.0\u001b[39m*init_phases, \u001b[32m0.0\u001b[39m*weights_data, \u001b[32m0.0\u001b[39m*weights_data\n\u001b[32m     56\u001b[39m ]\n\u001b[32m     57\u001b[39m b = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m carry, loss_mem = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mblock_until_ready\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m e = time.time()\n\u001b[32m     60\u001b[39m trained_phases = carry[\u001b[32m0\u001b[39m]\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/qml/venv/lib/python3.13/site-packages/jax/_src/pjit.py:337\u001b[39m, in \u001b[36m_cpp_pjit.<locals>.cache_miss\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.no_tracing.value:\n\u001b[32m    333\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mre-tracing function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjit_info.fun_sourceinfo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    334\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33m`jit`, but \u001b[39m\u001b[33m'\u001b[39m\u001b[33mno_tracing\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is set\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    336\u001b[39m (outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked, executable,\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m  pgle_profiler) = \u001b[43m_python_pjit_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m maybe_fastpath_data = _get_fastpath_data(\n\u001b[32m    340\u001b[39m     executable, out_tree, args_flat, out_flat, attrs_tracked, jaxpr.effects,\n\u001b[32m    341\u001b[39m     jaxpr.consts, jit_info.abstracted_axes,\n\u001b[32m    342\u001b[39m     pgle_profiler)\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outs, maybe_fastpath_data, _need_to_rebuild_with_fdo(pgle_profiler)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/qml/venv/lib/python3.13/site-packages/jax/_src/pjit.py:195\u001b[39m, in \u001b[36m_python_pjit_helper\u001b[39m\u001b[34m(fun, jit_info, *args, **kwargs)\u001b[39m\n\u001b[32m    193\u001b[39m   args_flat = \u001b[38;5;28mmap\u001b[39m(core.full_lower, args_flat)\n\u001b[32m    194\u001b[39m   core.check_eval_args(args_flat)\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m   out_flat, compiled, profiler = \u001b[43m_pjit_call_impl_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    197\u001b[39m   out_flat = pjit_p.bind(*args_flat, **p.params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/qml/venv/lib/python3.13/site-packages/jax/_src/pjit.py:1672\u001b[39m, in \u001b[36m_pjit_call_impl_python\u001b[39m\u001b[34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, name, keep_unused, inline, compiler_options_kvs, *args)\u001b[39m\n\u001b[32m   1660\u001b[39m compiler_options_kvs = compiler_options_kvs + \u001b[38;5;28mtuple\u001b[39m(pgle_compile_options.items())\n\u001b[32m   1661\u001b[39m \u001b[38;5;66;03m# Passing mutable PGLE profile here since it should be extracted by JAXPR to\u001b[39;00m\n\u001b[32m   1662\u001b[39m \u001b[38;5;66;03m# initialize the fdo_profile compile option.\u001b[39;00m\n\u001b[32m   1663\u001b[39m compiled = \u001b[43m_resolve_and_lower\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1664\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_shardings\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1665\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_layouts\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_layouts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1666\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_layouts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_layouts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_env\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresource_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1667\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1668\u001b[39m \u001b[43m    \u001b[49m\u001b[43minline\u001b[49m\u001b[43m=\u001b[49m\u001b[43minline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlowering_platforms\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1669\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlowering_parameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlir\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLoweringParameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1672\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1674\u001b[39m \u001b[38;5;66;03m# This check is expensive so only do it if enable_checks is on.\u001b[39;00m\n\u001b[32m   1675\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compiled._auto_spmd_lowering \u001b[38;5;129;01mand\u001b[39;00m config.enable_checks.value:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/qml/venv/lib/python3.13/site-packages/jax/_src/interpreters/pxla.py:2415\u001b[39m, in \u001b[36mMeshComputation.compile\u001b[39m\u001b[34m(self, compiler_options)\u001b[39m\n\u001b[32m   2413\u001b[39m compiler_options_kvs = \u001b[38;5;28mself\u001b[39m._compiler_options_kvs + t_compiler_options\n\u001b[32m   2414\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._executable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m compiler_options_kvs:\n\u001b[32m-> \u001b[39m\u001b[32m2415\u001b[39m   executable = \u001b[43mUnloadedMeshExecutable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_hlo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2416\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_hlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompile_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2417\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2418\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m compiler_options_kvs:\n\u001b[32m   2419\u001b[39m     \u001b[38;5;28mself\u001b[39m._executable = executable\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/qml/venv/lib/python3.13/site-packages/jax/_src/interpreters/pxla.py:2923\u001b[39m, in \u001b[36mUnloadedMeshExecutable.from_hlo\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   2920\u001b[39m       \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   2922\u001b[39m util.test_event(\u001b[33m\"\u001b[39m\u001b[33mpxla_cached_compilation\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2923\u001b[39m xla_executable = \u001b[43m_cached_compilation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2924\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspmd_lowering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2925\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtuple_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_spmd_lowering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_prop_to_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2926\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_prop_to_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpmap_nreps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2927\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2929\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m auto_spmd_lowering:\n\u001b[32m   2930\u001b[39m   \u001b[38;5;28;01massert\u001b[39;00m mesh \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/qml/venv/lib/python3.13/site-packages/jax/_src/interpreters/pxla.py:2729\u001b[39m, in \u001b[36m_cached_compilation\u001b[39m\u001b[34m(computation, name, mesh, spmd_lowering, tuple_args, auto_spmd_lowering, allow_prop_to_inputs, allow_prop_to_outputs, host_callbacks, backend, da, pmap_nreps, compiler_options_kvs, pgle_profiler)\u001b[39m\n\u001b[32m   2721\u001b[39m compile_options = create_compile_options(\n\u001b[32m   2722\u001b[39m     computation, mesh, spmd_lowering, tuple_args, auto_spmd_lowering,\n\u001b[32m   2723\u001b[39m     allow_prop_to_inputs, allow_prop_to_outputs, backend,\n\u001b[32m   2724\u001b[39m     dev, pmap_nreps, compiler_options)\n\u001b[32m   2726\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m dispatch.log_elapsed_time(\n\u001b[32m   2727\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFinished XLA compilation of \u001b[39m\u001b[38;5;132;01m{fun_name}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{elapsed_time:.9f}\u001b[39;00m\u001b[33m sec\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2728\u001b[39m     fun_name=name, event=dispatch.BACKEND_COMPILE_EVENT):\n\u001b[32m-> \u001b[39m\u001b[32m2729\u001b[39m   xla_executable = \u001b[43mcompiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile_or_get_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2730\u001b[39m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2731\u001b[39m \u001b[43m      \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2732\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m xla_executable\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/qml/venv/lib/python3.13/site-packages/jax/_src/compiler.py:452\u001b[39m, in \u001b[36mcompile_or_get_cached\u001b[39m\u001b[34m(backend, computation, devices, compile_options, host_callbacks, pgle_profiler)\u001b[39m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    451\u001b[39m   log_persistent_cache_miss(module_name, cache_key)\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_and_write_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m      \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/qml/venv/lib/python3.13/site-packages/jax/_src/compiler.py:653\u001b[39m, in \u001b[36m_compile_and_write_cache\u001b[39m\u001b[34m(backend, computation, compile_options, host_callbacks, module_name, cache_key)\u001b[39m\n\u001b[32m    644\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_compile_and_write_cache\u001b[39m(\n\u001b[32m    645\u001b[39m     backend: xc.Client,\n\u001b[32m    646\u001b[39m     computation: ir.Module,\n\u001b[32m   (...)\u001b[39m\u001b[32m    650\u001b[39m     cache_key: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    651\u001b[39m ) -> xc.LoadedExecutable:\n\u001b[32m    652\u001b[39m   start_time = time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m   executable = \u001b[43mbackend_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m   compile_time = time.monotonic() - start_time\n\u001b[32m    657\u001b[39m   _cache_write(\n\u001b[32m    658\u001b[39m       cache_key, compile_time, module_name, backend, executable, host_callbacks\n\u001b[32m    659\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/qml/venv/lib/python3.13/site-packages/jax/_src/profiler.py:333\u001b[39m, in \u001b[36mannotate_function.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    332\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, **decorator_kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    334\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/qml/venv/lib/python3.13/site-packages/jax/_src/compiler.py:303\u001b[39m, in \u001b[36mbackend_compile\u001b[39m\u001b[34m(backend, module, options, host_callbacks)\u001b[39m\n\u001b[32m    297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m backend.compile(\n\u001b[32m    298\u001b[39m         built_c, compile_options=options, host_callbacks=host_callbacks\n\u001b[32m    299\u001b[39m     )\n\u001b[32m    300\u001b[39m   \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[32m    301\u001b[39m   \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[32m    302\u001b[39m   \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m xc.XlaRuntimeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    305\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m error_handler \u001b[38;5;129;01min\u001b[39;00m _XLA_RUNTIME_ERROR_HANDLERS:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Varying the depth of the circuit\n",
    "\n",
    "depth_list = [8, 16, 24, 32, 40]# 48, 56, 64]  # Example depths\n",
    "num_steps = 1000\n",
    "step_size = 1e-2\n",
    "# Load MNIST data for handwritten digits '3' and '5'\n",
    "# The dataset has 'x' rows, each representing an image\n",
    "# Each row has 'y' columns: The first y-1 columns each correspond to a pixel feature, and the last column is the label\n",
    "# A label of +1 represents the digit '3', and -1 represents the digit '5'\n",
    "\n",
    "file_path = r\"/Users/giancarloramirez/documents/qml/mnist_pca/mnist_3-5_8d_test.csv\"\n",
    "data_train = pd.read_csv(file_path)\n",
    "data_train = jnp.array(data_train)\n",
    "\n",
    "\n",
    "#final column in the data is for labels \n",
    "num_features = data_train.shape[1] -1 \n",
    "\n",
    "file_path_1 = r\"/Users/giancarloramirez/documents/qml/mnist_pca/mnist_3-5_8d_test.csv\"\n",
    "data_test = pd.read_csv(file_path_1)\n",
    "data_test = jnp.array(data_test)\n",
    "\n",
    "num_steps = 1000\n",
    "\n",
    "\n",
    "# Spliiting the data into training and testing sets\n",
    "\n",
    "train_set = data_train[:,:num_features]\n",
    "\n",
    "# Rescale the training set to the range [-pi/2, pi/2]\n",
    "train_set = rescale_data(train_set, min_val = -(np.pi)/2, max_val = (np.pi/2))\n",
    "\n",
    "# The training labels are +1 for 3 and -1 for 5. \n",
    "train_labels = data_train[:,num_features]\n",
    "\n",
    "# Not needed for now:\n",
    "#train_labels_one_hot = jnp.zeros(shape = train_set.shape)\n",
    "#train_labels_one_hot = train_labels_one_hot.at[tuple([jnp.arange(num_train),train_labels])].set(1)\n",
    "test_set = data_test[:,:num_features]\n",
    "# Rescale the test set to the range [-pi/2, pi/2]\n",
    "test_set = rescale_data(test_set, min_val = -(np.pi)/2, max_val = (np.pi/2))\n",
    "\n",
    "test_labels = data_test[:,num_features]\n",
    "\n",
    "results = []\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for depth in depth_list:\n",
    "    print(f\"Training with depth = {depth}\")\n",
    "    a = time.time()\n",
    "    init_phases = initialize_phases(depth, 2*num_features)\n",
    "    weights_data = jnp.ones(shape=[init_phases.shape[0], init_phases.shape[1]])\n",
    "    init = [\n",
    "        init_phases, train_set, train_labels, weights_data,\n",
    "        0.0*init_phases, 0.0*init_phases, 0.0*weights_data, 0.0*weights_data\n",
    "    ]\n",
    "    b = time.time()\n",
    "    carry, loss_mem = jax.block_until_ready(train)(init)\n",
    "    e = time.time()\n",
    "    trained_phases = carry[0]\n",
    "    trained_weights = carry[3]\n",
    "\n",
    "    train_loss_before = float(loss(init_phases, train_set, train_labels, weights_data))\n",
    "    train_loss_after = float(loss(trained_phases, train_set, train_labels, trained_weights))\n",
    "    test_loss_after = float(loss(trained_phases, test_set, test_labels, trained_weights))\n",
    "    train_time_min = (e-b)/60\n",
    "    \n",
    "    plt.plot(np.array(loss_mem[:,0]), np.array(loss_mem[:,1]), label=f\"Depth {depth}\")\n",
    "    \n",
    "    d = time.time()\n",
    "    total_time_min = (d-a)/60\n",
    "\n",
    "    results.append({\n",
    "        \"depth\": depth,\n",
    "        \"train_loss_before\": train_loss_before,\n",
    "        \"train_loss_after\": train_loss_after,\n",
    "        \"test_loss_after\": test_loss_after,\n",
    "        \"train_time_min\": train_time_min,\n",
    "        \"total_time_min\": total_time_min\n",
    "    })\n",
    "\n",
    "\n",
    "plt.xlim(0, num_steps)\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(f\"Learning Curves for Different Circuit Depths (Unitary dimension: {2*init_phases.shape[1]} x {2*init_phases.shape[1]})\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Create and print the results table\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False, float_format=\"%.4F\"))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6659f371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot final losses vs depth \n",
    "depth = [r[\"depth\"] for r in results]\n",
    "train_loss = [r[\"train_loss_after\"] for r in results]\n",
    "test_loss = [r[\"test_loss_after\"] for r in results]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(depth, train_loss, 'o-', label=\"Train loss\")\n",
    "plt.plot(depth, test_loss, 's-', label=\"Test loss\")\n",
    "plt.xlabel(\"Depth\")\n",
    "plt.ylabel(\"Final loss\")\n",
    "plt.title(f\"Final Loss vs Depth (Unitary dimension: 6 x 6)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb2a644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Varying the number of features\n",
    "\n",
    "feature_dims = list(range(3, 11))  # 3d to 10d\n",
    "depth = 32                         # Set your desired constant depth here\n",
    "num_steps = 1000\n",
    "step_size = 1e-2\n",
    "\n",
    "results = []\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for f_dims in feature_dims:\n",
    "    print(f\"Training with {f_dims} features (depth={depth})\")\n",
    "    a =time.time()\n",
    "    file_path_train = fr\"C:\\Users\\kl23874\\.spyder-py3\\mnist_pca\\mnist_3-5_{f_dims}d_train.csv\"\n",
    "    file_path_test  = fr\"C:\\Users\\kl23874\\.spyder-py3\\mnist_pca\\mnist_3-5_{f_dims}d_test.csv\"\n",
    "    data_train = pd.read_csv(file_path_train)\n",
    "    data_train = jnp.array(data_train)\n",
    "    data_test = pd.read_csv(file_path_test)\n",
    "    data_test = jnp.array(data_test)\n",
    "    num_features = data_train.shape[1] - 1\n",
    "\n",
    "    train_set = data_train[:, :num_features]\n",
    "    train_set = rescale_data(train_set, min_val=-(np.pi)/2, max_val=(np.pi/2))\n",
    "    train_labels = data_train[:, num_features]\n",
    "    test_set = data_test[:, :num_features]\n",
    "    test_set = rescale_data(test_set, min_val=-(np.pi)/2, max_val=(np.pi/2))\n",
    "    test_labels = data_test[:, num_features]\n",
    "\n",
    "    a = time.time()\n",
    "    init_phases = initialize_phases(depth, 2*num_features)\n",
    "    weights_data = jnp.ones(shape=[init_phases.shape[0], init_phases.shape[1]])\n",
    "    init = [\n",
    "        init_phases, train_set, train_labels, weights_data,\n",
    "        0.0*init_phases, 0.0*init_phases, 0.0*weights_data, 0.0*weights_data\n",
    "    ]\n",
    "    b = time.time()\n",
    "    carry, loss_mem = jax.block_until_ready(train)(init)\n",
    "    e = time.time()\n",
    "    trained_phases = carry[0]\n",
    "    trained_weights = carry[3]\n",
    "\n",
    "    train_loss_before = float(loss(init_phases, train_set, train_labels, weights_data))\n",
    "    train_loss_after = float(loss(trained_phases, train_set, train_labels, trained_weights))\n",
    "    test_loss_after = float(loss(trained_phases, test_set, test_labels, trained_weights))\n",
    "    train_time_min = (e-b)/60\n",
    "\n",
    "    plt.plot(np.array(loss_mem[:,0]), np.array(loss_mem[:,1]), label=f\"({2*f_dims} x {2*f_dims})\")\n",
    "\n",
    "\n",
    "    d = time.time()\n",
    "    total_time_min = (d-a)/60\n",
    "    results.append({\n",
    "        \"features\": f_dims,\n",
    "        \"train_loss_before\": train_loss_before,\n",
    "        \"train_loss_after\": train_loss_after,\n",
    "        \"test_loss_after\": test_loss_after,\n",
    "        \"train_time_min\": train_time_min,\n",
    "        \"total_time_min\": total_time_min\n",
    "    })\n",
    "\n",
    "plt.xlim(0, num_steps)\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(f\"Learning Curves for Different Feature Dimensions (Depth={depth}) - Multiphoton input\")\n",
    "\n",
    "plt.legend(title=\"Unitary dimension\")\n",
    "plt.show()\n",
    "\n",
    "# Create and print the results table\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False, float_format=\"%.4F\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8993b82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot final losses vs feature size\n",
    "features = [r[\"features\"] for r in results]\n",
    "train_loss = [r[\"train_loss_after\"] for r in results]\n",
    "test_loss = [r[\"test_loss_after\"] for r in results]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(features, train_loss, 'o-', label=\"Train loss\")\n",
    "plt.plot(features, test_loss, 's-', label=\"Test loss\")\n",
    "plt.xlabel(\"Feature size (d)\")\n",
    "plt.ylabel(\"Final loss\")\n",
    "plt.title(f\"Final Loss vs Feature Size (Depth={depth})\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fd5c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check multiphoton output combinations\n",
    "\n",
    "# Example usage of the measurement function\n",
    "n = 3  # number of matrices\n",
    "size = 6  # size of each identity matrix\n",
    "\n",
    "# Create a (n, 1, 1) array of factors: [0], [1], [2]\n",
    "factors = jnp.arange(n, dtype=jnp.complex64).reshape(-1, 1, 1)\n",
    "# Create a (1, 6, 6) identity matrix and broadcast\n",
    "ones_matrix = jnp.arange(n * size * size, dtype=jnp.complex64).reshape(n, size, size)\n",
    "# Multiply to get (n, 6, 6)\n",
    "temp_unitaries = factors * ones_matrix\n",
    "\n",
    "#print(temp_unitaries.shape)  # (3, 6, 6)\n",
    "#print(temp_unitaries)\n",
    "\n",
    "result_measurement, combos1, probs1, _ = measurement(temp_unitaries, num_photons = 3)\n",
    "#print(probs1)\n",
    "#print(result_measurement.shape)  # Should be (num_samples, 2, 6) if num_samples is the batch size\n",
    "parity = jnp.sum(combos1, axis=1) % 2\n",
    "mask = (parity == 1)  # shape (n_combos,)\n",
    "\n",
    "arr = jnp.arange(1, 21)\n",
    "test = parity*arr\n",
    "test1 = mask*arr\n",
    "print(\"Masked array:\", test1)\n",
    "print(\"Even indices:\", test)\n",
    "print(parity)  # 0 = even, 1 = odd\n",
    "print(combos1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3523007",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check permanent calculation\n",
    "\n",
    "def perm_3x3_jax(mat):\n",
    "    # Only works for 3x3 matrices\n",
    "    perms = jnp.array([\n",
    "        [0, 1, 2],\n",
    "        [0, 2, 1],\n",
    "        [1, 0, 2],\n",
    "        [1, 2, 0],\n",
    "        [2, 0, 1],\n",
    "        [2, 1, 0]\n",
    "    ])\n",
    "    return jnp.sum(jnp.prod(mat[jnp.arange(3), perms], axis=1))\n",
    "\n",
    "# Example usage\n",
    "mat = jnp.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=jnp.complex64)\n",
    "result_perm = perm_3x3_jax(mat) \n",
    "print(\"Permanent of the matrix:\", result_perm)  # Should print the permanent of the matrix\n",
    "mat1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=jnp.complex64)\n",
    "print(\"Permanent of the matrix (using numpy):\", perm(mat1))  # For comparison with numpy's perm function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1315aa02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
